# 特征工程

对于一个机器学习问题，数据和特征往往决定了结果的上限，而模型、算法的选择以及优化则时逐步接近这个上限。

**特征工程**：就是对原始数据进行一系列的处理，将其提炼为特征，作为输入提供给算法和模型使用。旨在去除原始数据中的杂质和冗余信息。

*常见的数据类型*：结构化数据和非结构化数据

## 特征归一化

为了消除数据特征之间的量纲影响，我们需要对特征进行归一化处理，使得不同的指标之间具有可比性。

* ***知识点***(★☆☆☆☆)

    * 特征归一化

* ***问题***

    * <font color=red>为什么需要对数值类型的特征做归一化？</font>

* ***分析与解答***

    对数值类型的特征做归一化可以将所有的特征都统一到一个大致相同的数值区间内

    * 线性函数归一化(Min-Max Scaling)

        对原始数据进行线性变换，使结果映射到[0，1]的范围，实现对原始数据的等比缩放
        $$
        X_{norm} = \frac{X-X_{min}}{X_{max}-X_{min}}
        $$

        * 其中$X$为原始数据，$X_{max}$、$X_{min}$分别为数据的最大值和最小值
        
    * 零均值归一化(Z-score Normalization)
    
        将原始数据映射到均值为0、标准差为1的分布上
        $$
        z = \frac{x-\mu}{\sigma}
        $$
    
        * 均值为$\mu$、标准差为$\sigma$

## 类别型特征

类别型特征原始市场通常是字符串形式，除了决策树等少数模型能直接处理字符串形式的输入，对于大多数模型来说，类别特征剥削经过处理转换为数值型特征才可以。

* ***知识点***(★★☆☆☆)

    * 序号编码(Ordinal Encoding)
    * 独热编码(One-hot Encoding)
    * 二进制编码(Binary Encoding)

* **问题**

    * <font color=red>在对数据进行预处理时，应该怎样处理类别型特征？</font>

* ***分析与解答***

    * 序号编码

        序号编码常用于处理类别间具有大小关系的数据。例如成绩，可以分为低、中、高三 档，并且存在“高>中>低”的排序关系。序号编码会按照大小关系对类别型特征赋予一个数值 ID，例如高表示为3、中表示为2、低表示为1，转换后依然保留了大小关系

    * 独热编码

        ​		独热编码通常用于处理类别间不具有大小关系的特征。例如血型，一共有4个取值（A型 血、B型血、AB型血、O型血），独热编码会把血型变成一个4维稀疏向量，A型血表示为 （1, 0, 0, 0），B型血表示为（0, 1, 0, 0），AB型表示为（0, 0, 1, 0），O型血表示为（0, 0, 0, 1）。对于类别取值较多的情况下使用独热编码需要注意以下问题：

        * 使用稀疏向量来节省空间。在独热编码下，特征向量只有某一维取值为1，其他位 置取值均为0。因此可以利用向量的稀疏表示有效地节省空间，并且目前大部分的算法均接 受稀疏向量形式的输入
        * 配合特征选择来降低维度。高维度特征会带来几方面的问题。
            * 一是在K近邻算法中，高维空间下两点之间的距离很难得到有效的衡量
            * 二是在逻辑回归模型中，参数的数量会随着维度的增高而增加，容易引起过拟合问题
            * 三是通常只有部分维度是对分类、预测有帮助，因此可以考虑配合特征选择来降低维度

    * 二进制编码

        二进制编码主要分为两步，先用序号编码给每个类别赋予一个类别ID，然后将类别ID对应的二进制编码作为结果。以A、B、AB、O血型为例，下表是二进制编码的过程。A型血的ID为1，二进制表示为001；B型血的ID为2，二进制表示为010；以此类推可以得到AB型血和O型血的二进制表示。可以看出，二进制编码本质上是利用二进制对ID进行哈希映射，最终得到0/1特征向量，且维数少于独热编码，节省了存储空间。

        ![](.\img\二进制编码与独热编码.png "二进制编码与独热编码")

        

## 高维组合特征的处理

* ***知识点***(★★☆☆☆)

    * 组合特征

* ***问题***

    * <font color=red>什么时组合特征？如何处理高维组合特征？</font>

* ***分析与解答***

    为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。

    * 例：以逻辑回归为例，假设数据的特征向量为X=(x1,x2,...,xk )，则有：
        $$
        Y = sigmoid(\sum\limits_{i}\sum\limits_{j}w_{ij}<x_i,y_j>)
        $$
        其中$<x_i,y_j>$表示$x_i$和$x_j$的组合特征，$w_{ij}$的维度等于$|x_i|.|x_j|$分别代表第$i$个特征和第$j$个特征不同取值个数。

    

    若用户的数量为$m$、物品的数量为$n$，那么需要学习的参数的规模为$m×n$。在互联网环境 下，用户数量和物品数量都可以达到千万量级，几乎无法学习$m×n$规模的参数。在这种情况 下，一种行之有效的方法是将用户和物品分别用k维的低维向量表示$(k<<m,k<<n)$,
    $$
    Y = sigmoid(\sum\limits_{i}\sum\limits_{j}w_{ij}<x_i,y_j>)
    $$
    其中 ，$w_{ij} = x_{i}^{'}.x_{j}^{'}$,$x_{i}^{'}$和$x_{j}^{'}$分别表示$x_i$和$x_j$对应的低维向量。这其实等价于矩阵分解。

## 文本表示模型

* ***知识点***(★★☆☆☆)

    * 词袋模型(Bag Words)
    * TF-IDF(Term Frequency-Inverse Document)
    * 主题模型(Topic Mode)
    * 词嵌入模型(Word Embedding)

* ***问题***

    * <font color=red>有哪些文本表示模型？它们各有什么优点?</font>

* ***分析与解答***

    * 词袋模型和N-gram模型

        ​		最基础的文本表示模型是词袋模型。顾名思义，就是将每篇文章看成一袋子词，并忽略 每个词出现的顺序。具体地说，就是将整段文本以词为单位切分开，然后每篇文章可以表示 成一个长向量，向量中的每一维代表一个单词，而该维对应的权重则反映了这个词在原文章 中的重要程度。常用TF-IDF来计算权重，公式为
        $$
        TF-IDF(t,d) = tf(t,d)\times IDF(t)
        $$
        其中$TF(t,d)$为单纯$t$在文档$d$中出现的频率，$IDF(t)$是逆文档频率，用来衡量单词$t$对表达语义所起的重要性，表示为
        $$
        IDF(t) = log\frac{文章总数}{包含单词t的文章总数+1}
        $$
        直观的解释是，如果一个单词在非常多的文章里面都出现，那么它可能是一个比较通用的词 汇，对于区分某篇文章特殊语义的贡献较小，因此对权重做一定惩罚。

        ​		将文章进行单词级别的划分有时候并不是一种好的做法，比如英文中的natural language processing（自然语言处理）一词，如果将natural，language，processing这3个词拆分开来， 所表达的含义与三个词连续出现时大相径庭。通常，可以将连续出现的n个词（n≤N）组成的 词组（N-gram）也作为一个单独的特征放到向量表示中去，构成N-gram模型。另外，同一个词可能有多种词性变化，却具有相似的含义。在实际应用中，一般会对单词进行词干抽取（Word Stemming）处理，即将不同词性的单词统一成为同一词干的形式。

    * 主题模型

        主题模型用于从文本库中发现有代表性的主题（得到每个主题上面词的分布特性），并 且能够计算出每篇文章的主题分布，具体细节参见**概率图模型$\rightarrow$主题模型。**

    * 词嵌入与深度学习模型

        ​		词嵌入是一类将词向量化的模型的统称，核心思想是将每个词都映射成低维空间（通常K=50～300维）上的一个稠密向量（Dense Vector）。K维空间的每一维也可以看作一个隐含的主题，只不过不像主题模型中的主题那样直观。

        ​		由于词嵌入将每个词映射成一个K维的向量，如果一篇文档有N个词，就可以用一个N×K 维的矩阵来表示这篇文档，但是这样的表示过于底层。在实际应用中，如果仅仅把这个矩阵 作为原文本的表示特征输入到机器学习模型中，通常很难得到令人满意的结果。因此，还需要在此基础之上加工出更高层的特征。在传统的浅层机器学习模型中，一个好的特征工程往往可以带来算法效果的显著提升。而深度学习模型正好为我们提供了一种自动地进行特征工程的方式，模型中的每个隐层都可以认为对应着不同抽象层次的特征。从这个角度来讲，深度学习模型能够打败浅层模型也就顺理成章了。卷积神经网络和循环神经网络的结构在文本 表示中取得了很好的效果，主要是由于它们能够更好地对文本进行建模，抽取出一些高层的语义特征。与全连接的网络结构相比，卷积神经网络和循环神经网络一方面很好地抓住了文本的特性，另一方面又减少了网络中待学习的参数，提高了训练速度，并且降低了过拟合的风险。

## Word2Vec

WordVec是谷歌2013年提出来的，母亲是最常用的词嵌入模型之一，WordVec实际是一种浅层神经网络模型，它有两种网络结构：CBOW(Continues Bag of Words)、Skipgram

* ***知识点***(★★★☆☆)

    * WordVec
    * 隐狄利克雷模型(LDA)
    * CBOW
    * Skip-gram

* ***问题***

    * <font color=red>Word2Vec是如何工作的？它和LDA有什么区别与联系？</font>

* ***分析与解答***

    ​		CBOW的目标是根据上下文出现的词语来预测当前词的生成概率，如图a；而Skip-gram是根据当前词来预测上下文中各词的生成概率，如图b。

    ![Word2Vec的两种网络结构](.\img\Word2Vec的两种网络结构.png "Word2Vec的两种网络结构")

    ​		其中w(t)是当前所关注的词，w(t−2)、w(t−1)、w(t+1)、w(t+2)是上下文中出现的词。这里前后滑动窗口大小均设为2。

    ​		CBOW和Skip-gram都可以表示成由输入层（Input）、映射层（Projection）和输出层 （Output）组成的神经网络。

    ​		输入层中的每个词由独热编码方式表示，即所有词均表示成一个N维向量，其中N为词汇表中单词的总数。在向量中，每个词都将与之对应的维度置为1，其余维度的值均设为0。 

    ​		在映射层（又称隐含层）中，K个隐含单元（Hidden Units）的取值可以由N维输入向量以及连接输入和隐含单元之间的N×K维权重矩阵计算得到。在CBOW中，还需要将各个输入词所计算出的隐含单元求和。 

    ​		在输出层向量的值可以通过隐含层向量（K维），以及连接隐含层和输出层之间的 K×N维权重矩阵计算得到。输出层也是一个N维向量，每维与词汇表中的一个单词相对应。最后，对输出层向量应用Softmax激活函数，可以计算出每个单词的生成概率。Softmax激活函数的定义为：
    $$
    P(y=w_n|x)=\frac{e^{x_n}}{\sum\limits_{k=1}^{N}e^{x_k}}
    $$
    其中$x$代表N维的原始输出向量，$x_n$为在原始输出向量中，与单词$w_n$所对应维度的取值。

    ​		接下来的任务就是训练神经网络的权重，使得语料库中所有单词的整体生成概率最大化。从输入层到隐含层需要一个维度为N×K的权重矩阵，从隐含层到输出层又需要一个维度为K×N的权重矩阵，学习权重可以用反向传播算法实现，每次迭代时将权重沿梯度更优的方向进行一小步更新。但是由于Softmax激活函数中存在归一化项的缘故，推导出来的迭代公式需要对词汇表中的所有单词进行遍历，使得每次迭代过程非常缓慢，由此产生了Hierarchical Softmax和Negative Sampling两种改进方法。训练得到维度为N×K和K×N的两个权重矩阵之后，可以选择其中一个作为N个词的K维向量表示。

    ​		谈到Word2Vec与LDA的区别和联系，首先，LDA是利用文档中单词的共现关系来对单词按主题聚类，也可以理解为对“文档-单词”矩阵进行分解，得到“文档-主题”和“主题-单词”两个概率分布。而Word2Vec其实是对“上下文-单词”矩阵进行学习，其中上下文由周围的几个单词组成，由此得到的词向量表示更多地融入了上下文共现的特征。也就是说，如果两个单词所对应的Word2Vec向量相似度较高，那么它们很可能经常在同样的上下文中出现。需要说明的是，上述分析的是LDA与Word2Vec的不同，不应该作为主题模型和词嵌入两类方法的主要差异。主题模型通过一定的结构调整可以基于“上下文-单词”矩阵进行主题推理。同样地，词嵌入方法也可以根据“文档-单词”矩阵学习出词的隐含向量表示。主题模型和词嵌入两类方法最大的不同其实在于模型本身，主题模型是一种基于概率图模型的生成式模型，其似然函数可以写成若干条件概率连乘的形式，其中包括需要推测的隐含变量（即主题）；而 词嵌入模型一般表达为神经网络的形式，似然函数定义在网络的输出之上，需要通过学习网络的权重以得到单词的稠密向量表示。

    

## 图像数据不足时的处理办法

* ***知识点***(★★★☆☆)

    * 迁移学习（Transfer Learning）
    * 生成对抗网络
    * 图像处理
    * 上采样技术
    * 数据扩充

* ***问题***

    * <font color=red>在图像分类任务中，训练数据不足会带来什么问题？如何缓解数据量不足带来的问题？</font>

* ***分析与解答***

    ​		一个模型所能提供的信息一般来源于两个方面，一是训练数据中蕴含的信息；二是在模型的形成过程中(包括构造、学习、推理等)，人们提供的先验信息。当训练数据不足时，说明模型从原始数据中获取的信息比较少，这种情况下要想保证模型的效果，就需要更多先验信息。先验信息可以作用在模型上，例如让模型采用特定的内在结构、条件假设或添加其他一些约束条件；先验信息也可以直接施加在数据集上，即根据特定的先验假设去调整、变换或扩展训练数据，让其展现出更多的、更有用的信息，以利于后续模型的训练和学习。

    ​		具体到图像分类任务上，训练数据不足带来的问题主要表现在过拟合方面，即模型在训练样本上的效果可能不错，但在测试集上的泛化效果不佳。根据上述讨论，对应的处理方法大致也可以分两类，一是基于模型的方法，主要是采用降低过拟合风险的措施，包括简化模型（如将非线性模型简化为线性模型）、添加约束项以缩小假设空间（如$L_1/L_2$正则项）、 集成学习、Dropout超参数等；二是基于数据的方法，主要通过数据扩充（Data Augmentation），即根据一些先验知识，在保持特定信息的前提下，对原始数据进行适当变换以达到扩充数据集的效果。具体到图像分类任务中，在保持图像类别不变的前提下，可以对训练集中的每幅图像进行以下变换:

    * 一定程度内的随机旋转、平移、缩放、裁剪、填充、左右翻转等，这些变换对应着同一个目标在不同角度的观察结果。 
    * 对图像中的像素添加噪声扰动,比如椒盐噪声、高斯白噪声等。 
    * 颜色变换。例如，在图像的RGB颜色空间上进行主成分分析，得到3个主成分的特征向量$p_1,p_2,p_3$及其对应的特征值 $λ_1,λ_2,λ_3$，然后在每个像素的RGB值上添加增量 $[p_1,p_2,p_3]•[α_1λ_1,α_2λ_2,α_3λ_3]^T$，其中 $α_1,α_2,α_3$是均值为0、方差较小的高斯分布随机数。 
    * 改变图像的亮度、清晰度、对比度、锐度等。

    ​		除了直接在图像空间进行变换，还可以先对图像进行特征提取，然后在图像的特征空间内进行变换，利用一些通用的数据扩充或上采样技术，例如SMOTE(Synthetic Minority Over-sampling Technique)算法。抛开上述这些启发式的变换方法，使用生成模型也可以合成一些新样本，例如当今非常流行的生成式对抗网络模型。

    ​		此外，借助已有的其他模型或数据来进行迁移学习在深度学习中也十分常见。例如，对 于大部分图像分类任务，并不需要从头开始训练模型，而是借用一个在大规模数据集上预训 练好的通用模型，并在针对目标任务的小数据集上进行微调（fine-tune），这种微调操作就可以看成是一种简单的迁移学习。

# 模型评估

## 评价指标的局限性

## ROC曲线

## 余弦距离的应用

## A/B测试的陷阱

## 模型评估的方法

## 超参数调优

## 过拟合与欠拟合

# 经典算法

## 支持向量机

## 逻辑回归

## 决策树

# 降维

## PCA最大化方差理论

## PCA最小平方误差理论

## 线性判别分析

## 线性判别分析与主成分分析

# 非监督学习

## K均值聚类

## 高斯混合模型

## 自组织映射神经网络

## 聚类算法的评估

# 概率图模型

## 概率图模型的联合概率分布

## 概率图表示

## 生成式模型与判别式模型

## 马尔可夫模型

## 主题模型

# 优化算法

## 有监督学习的损失函数

## 机器学习中的优化问题

## 经典优化算法

## 梯度验证

## 随机梯度下降法

## 随机梯度下降法的加速

## L1正则化与稀疏性

# 采样

## 采样的作用

## 均匀分布随机数

## 常见的采样方法

## 高斯分布的采样

## 马尔可夫蒙特卡洛采样法

## 贝叶斯网络采样

## 不均衡样本的重采样

# 前向神经网络

## 多层感知机与布尔函数

## 深度神经网络中的激活函数

## 多层感知机的反向传播算法

## 神经网络训练技巧

## 深度卷积神经网络

## 深度残差网络

# 循环神经网络

## 循环升级网络和卷积神经网络

## 循环神经网络的梯度消失问题

## 循环神经网络中的激活函数

## 长短期记忆模型

## Seq2Seq模型

## 注意力机制

# 强化学习

## 强化学习基础

## 视频游戏里的强化学习

## 策略梯度

## 探索与利用

# 集成学习

## 集成学习的种类

## 集成学习的步骤和例子

## 基分类器

## 偏差与误差

## 梯度提升决策树的基本原理

## XGBoost与GBDT的联系与区别

# 生成对抗神经网络

## 初始GANs的密码

## WGAN：抓住低维的幽灵

## DCGAN：当GANs遇上卷积

## ALI：包揽推断业务

## IRGAN：生成离散样本

## SeqGAN：生成文本序列



