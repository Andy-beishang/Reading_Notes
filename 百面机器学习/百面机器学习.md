# 特征工程

对于一个机器学习问题，数据和特征往往决定了结果的上限，而模型、算法的选择以及优化则时逐步接近这个上限。

**特征工程**：就是对原始数据进行一系列的处理，将其提炼为特征，作为输入提供给算法和模型使用。旨在去除原始数据中的杂质和冗余信息。

*常见的数据类型*：结构化数据和非结构化数据

## 特征归一化

为了消除数据特征之间的量纲影响，我们需要对特征进行归一化处理，使得不同的指标之间具有可比性。

* ***知识点***(★☆☆☆☆)

    * 特征归一化

* ***问题***

    * *为什么需要对数值类型的特征做归一化？*

* ***分析与解答***

    对数值类型的特征做归一化可以将所有的特征都统一到一个大致相同的数值区间内

    * 线性函数归一化(Min-Max Scaling)

        对原始数据进行线性变换，使结果映射到[0，1]的范围，实现对原始数据的等比缩放
        $$
        X_{norm} = \frac{X-X_{min}}{X_{max}-X_{min}}
        $$

        * 其中$X$为原始数据，$X_{max}$、$X_{min}$分别为数据的最大值和最小值
        
    * 零均值归一化(Z-score Normalization)
    
        将原始数据映射到均值为0、标准差为1的分布上
        $$
        z = \frac{x-\mu}{\sigma}
        $$
    
        * 均值为$\mu$、标准差为$\sigma$

## 类别型特征

类别型特征原始市场通常是字符串形式，除了决策树等少数模型能直接处理字符串形式的输入，对于大多数模型来说，类别特征剥削经过处理转换为数值型特征才可以。

* ***知识点***(★★☆☆☆)

    * 序号编码(Ordinal Encoding)
    * 独热编码(One-hot Encoding)
    * 二进制编码(Binary Encoding)

* ***问题**

    * 在对数据进行预处理时，应该怎样处理类别型特征？

* ***分析与解答***

    * 序号编码

        序号编码常用于处理类别间具有大小关系的数据。例如成绩，可以分为低、中、高三 档，并且存在“高>中>低”的排序关系。序号编码会按照大小关系对类别型特征赋予一个数值 ID，例如高表示为3、中表示为2、低表示为1，转换后依然保留了大小关系

    * 独热编码

        ​		独热编码通常用于处理类别间不具有大小关系的特征。例如血型，一共有4个取值（A型 血、B型血、AB型血、O型血），独热编码会把血型变成一个4维稀疏向量，A型血表示为 （1, 0, 0, 0），B型血表示为（0, 1, 0, 0），AB型表示为（0, 0, 1, 0），O型血表示为（0, 0, 0, 1）。对于类别取值较多的情况下使用独热编码需要注意以下问题：

        * 使用稀疏向量来节省空间。在独热编码下，特征向量只有某一维取值为1，其他位 置取值均为0。因此可以利用向量的稀疏表示有效地节省空间，并且目前大部分的算法均接 受稀疏向量形式的输入
        * 配合特征选择来降低维度。高维度特征会带来几方面的问题。
            * 一是在K近邻算法中，高维空间下两点之间的距离很难得到有效的衡量
            * 二是在逻辑回归模型中，参数的数量会随着维度的增高而增加，容易引起过拟合问题
            * 三是通常只有部分维度是对分类、预测有帮助，因此可以考虑配合特征选择来降低维度

    * 二进制编码

        二进制编码主要分为两步，先用序号编码给每个类别赋予一个类别ID，然后将类别ID对应的二进制编码作为结果。以A、B、AB、O血型为例，下表是二进制编码的过程。A型血的ID为1，二进制表示为001；B型血的ID为2，二进制表示为010；以此类推可以得到AB型血和O型血的二进制表示。可以看出，二进制编码本质上是利用二进制对ID进行哈希映射，最终得到0/1特征向量，且维数少于独热编码，节省了存储空间。

        ![](.\img\二进制编码与独热编码.png "二进制编码与独热编码")

        

## 高维组合特征的处理

* ***知识点***(★★☆☆☆)

    * 组合特征

* ***问题***

    * 什么时组合特征？如何处理高维组合特征？

* ***分析与解答***

    为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。

    * 例：以逻辑回归为例，假设数据的特征向量为X=(x1,x2,...,xk )，则有：
        $$
        Y = sigmoid(\sum\limits_{i}\sum\limits_{j}w_{ij}<x_i,y_j>)
        $$
        其中$<x_i,y_j>$表示$x_i$和$x_j$的组合特征，$w_{ij}$的维度等于$|x_i|.|x_j|$分别代表第$i$个特征和第$j$个特征不同取值个数。

    

    若用户的数量为$m$、物品的数量为$n$，那么需要学习的参数的规模为$m×n$。在互联网环境 下，用户数量和物品数量都可以达到千万量级，几乎无法学习$m×n$规模的参数。在这种情况 下，一种行之有效的方法是将用户和物品分别用k维的低维向量表示$(k<<m,k<<n)$,
    $$
    Y = sigmoid(\sum\limits_{i}\sum\limits_{j}w_{ij}<x_i,y_j>)
    $$
    其中 ，$w_{ij} = x_{i}^{'}.x_{j}^{'}$,$x_{i}^{'}$和$x_{j}^{'}$分别表示$x_i$和$x_j$对应的低维向量。这其实等价于矩阵分解。

## 文本表示模型

## Word2Vec

## 图像数据不足时的处理办法

# 模型评估

## 评价指标的局限性

## ROC曲线

## 余弦距离的应用

## A/B测试的陷阱

## 模型评估的方法

## 超参数调优

## 过拟合与欠拟合

# 经典算法

## 支持向量机

## 逻辑回归

## 决策树

# 降维

## PCA最大化方差理论

## PCA最小平方误差理论

## 线性判别分析

## 线性判别分析与主成分分析

# 非监督学习

## K均值聚类

## 高斯混合模型

## 自组织映射神经网络

## 聚类算法的评估

# 概率图模型

## 概率图模型的联合概率分布

## 概率图表示

## 生成式模型与判别式模型

## 马尔可夫模型

## 主题模型

# 优化算法

## 有监督学习的损失函数

## 机器学习中的优化问题

## 经典优化算法

## 梯度验证

## 随机梯度下降法

## 随机梯度下降法的加速

## L1正则化与稀疏性

# 采样

## 采样的作用

## 均匀分布随机数

## 常见的采样方法

## 高斯分布的采样

## 马尔可夫蒙特卡洛采样法

## 贝叶斯网络采样

## 不均衡样本的重采样

# 前向神经网络

## 多层感知机与布尔函数

## 深度神经网络中的激活函数

## 多层感知机的反向传播算法

## 神经网络训练技巧

## 深度卷积神经网络

## 深度残差网络

# 循环神经网络

## 循环升级网络和卷积神经网络

## 循环神经网络的梯度消失问题

## 循环神经网络中的激活函数

## 长短期记忆模型

## Seq2Seq模型

## 注意力机制

# 强化学习

## 强化学习基础

## 视频游戏里的强化学习

## 策略梯度

## 探索与利用

# 集成学习

## 集成学习的种类

## 集成学习的步骤和例子

## 基分类器

## 偏差与误差

## 梯度提升决策树的基本原理

## XGBoost与GBDT的联系与区别

# 生成对抗神经网络

## 初始GANs的密码

## WGAN：抓住低维的幽灵

## DCGAN：当GANs遇上卷积

## ALI：包揽推断业务

## IRGAN：生成离散样本

## SeqGAN：生成文本序列



