# 衡量特征工程

对于一个机器学习问题，数据和特征往往决定了结果的上限，而模型、算法的选择以及优化则时逐步接近这个上限。

**特征工程**：就是对原始数据进行一系列的处理，将其提炼为特征，作为输入提供给算法和模型使用。旨在去除原始数据中的杂质和冗余信息。

*常见的数据类型*：结构化数据和非结构化数据

## 特征归一化

为了消除数据特征之间的量纲影响，我们需要对特征进行归一化处理，使得不同的指标之间具有可比性。

* ***知识点***

    * 特征归一化

* ***问题***(★☆☆☆☆)

    * <font color=red>为什么需要对数值类型的特征做归一化？</font>

* ***分析与解答***

    对数值类型的特征做归一化可以将所有的特征都统一到一个大致相同的数值区间内

    * 线性函数归一化(Min-Max Scaling)

        对原始数据进行线性变换，使结果映射到[0，1]的范围，实现对原始数据的等比缩放
        $$
        X_{norm} = \frac{X-X_{min}}{X_{max}-X_{min}}
        $$

        * 其中$X$为原始数据，$X_{max}$、$X_{min}$分别为数据的最大值和最小值

    * 零均值归一化(Z-score Normalization)

        将原始数据映射到均值为0、标准差为1的分布上
        $$
        z = \frac{x-\mu}{\sigma}
        $$

        * 均值为$\mu$、标准差为$\sigma$

## 类别型特征

类别型特征原始市场通常是字符串形式，除了决策树等少数模型能直接处理字符串形式的输入，对于大多数模型来说，类别特征剥削经过处理转换为数值型特征才可以。

* ***知识点***

    * 序号编码(Ordinal Encoding)
    * 独热编码(One-hot Encoding)
    * 二进制编码(Binary Encoding)

* ***问题***(★★☆☆☆)

    * <font color=red>在对数据进行预处理时，应该怎样处理类别型特征？</font>

* ***分析与解答***

    * 序号编码

        序号编码常用于处理类别间具有大小关系的数据。例如成绩，可以分为低、中、高三 档，并且存在“高>中>低”的排序关系。序号编码会按照大小关系对类别型特征赋予一个数值 ID，例如高表示为3、中表示为2、低表示为1，转换后依然保留了大小关系

    * 独热编码

        ​		独热编码通常用于处理类别间不具有大小关系的特征。例如血型，一共有4个取值（A型 血、B型血、AB型血、O型血），独热编码会把血型变成一个4维稀疏向量，A型血表示为 （1, 0, 0, 0），B型血表示为（0, 1, 0, 0），AB型表示为（0, 0, 1, 0），O型血表示为（0, 0, 0, 1）。对于类别取值较多的情况下使用独热编码需要注意以下问题：

        * 使用稀疏向量来节省空间。在独热编码下，特征向量只有某一维取值为1，其他位 置取值均为0。因此可以利用向量的稀疏表示有效地节省空间，并且目前大部分的算法均接 受稀疏向量形式的输入
        * 配合特征选择来降低维度。高维度特征会带来几方面的问题。
            * 一是在K近邻算法中，高维空间下两点之间的距离很难得到有效的衡量
            * 二是在逻辑回归模型中，参数的数量会随着维度的增高而增加，容易引起过拟合问题
            * 三是通常只有部分维度是对分类、预测有帮助，因此可以考虑配合特征选择来降低维度

    * 二进制编码

        二进制编码主要分为两步，先用序号编码给每个类别赋予一个类别ID，然后将类别ID对应的二进制编码作为结果。以A、B、AB、O血型为例，下表是二进制编码的过程。A型血的ID为1，二进制表示为001；B型血的ID为2，二进制表示为010；以此类推可以得到AB型血和O型血的二进制表示。可以看出，二进制编码本质上是利用二进制对ID进行哈希映射，最终得到0/1特征向量，且维数少于独热编码，节省了存储空间。

        ![](.\img\二进制编码与独热编码.png "二进制编码与独热编码")

        

## 高维组合特征的处理

* ***知识点***(★★☆☆☆)

    * 组合特征

* ***问题***(★★☆☆☆)

    * <font color=red>什么时组合特征？如何处理高维组合特征？</font>

* ***分析与解答***

    为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。

    * 例：以逻辑回归为例，假设数据的特征向量为X=(x1,x2,...,xk )，则有：
        $$
        Y = sigmoid(\sum\limits_{i}\sum\limits_{j}w_{ij}<x_i,y_j>)
        $$
        其中$<x_i,y_j>$表示$x_i$和$x_j$的组合特征，$w_{ij}$的维度等于$|x_i|.|x_j|$分别代表第$i$个特征和第$j$个特征不同取值个数。

    

    若用户的数量为$m$、物品的数量为$n$，那么需要学习的参数的规模为$m×n$。在互联网环境 下，用户数量和物品数量都可以达到千万量级，几乎无法学习$m×n$规模的参数。在这种情况 下，一种行之有效的方法是将用户和物品分别用k维的低维向量表示$(k<<m,k<<n)$,
    $$
    Y = sigmoid(\sum\limits_{i}\sum\limits_{j}w_{ij}<x_i,y_j>)
    $$
    其中 ，$w_{ij} = x_{i}^{'}.x_{j}^{'}$,$x_{i}^{'}$和$x_{j}^{'}$分别表示$x_i$和$x_j$对应的低维向量。这其实等价于矩阵分解。

## 文本表示模型

* ***知识点***

    * 词袋模型(Bag Words)
    * TF-IDF(Term Frequency-Inverse Document)
    * 主题模型(Topic Mode)
    * 词嵌入模型(Word Embedding)

* ***问题***(★★☆☆☆)

    * <font color=red>有哪些文本表示模型？它们各有什么优点?</font>

* ***分析与解答***

    * 词袋模型和N-gram模型

        ​		最基础的文本表示模型是词袋模型。顾名思义，就是将每篇文章看成一袋子词，并忽略 每个词出现的顺序。具体地说，就是将整段文本以词为单位切分开，然后每篇文章可以表示 成一个长向量，向量中的每一维代表一个单词，而该维对应的权重则反映了这个词在原文章 中的重要程度。常用TF-IDF来计算权重，公式为
        $$
        TF-IDF(t,d) = tf(t,d)\times IDF(t)
        $$
        其中$TF(t,d)$为单纯$t$在文档$d$中出现的频率，$IDF(t)$是逆文档频率，用来衡量单词$t$对表达语义所起的重要性，表示为
        $$
        IDF(t) = log\frac{文章总数}{包含单词t的文章总数+1}
        $$
        直观的解释是，如果一个单词在非常多的文章里面都出现，那么它可能是一个比较通用的词 汇，对于区分某篇文章特殊语义的贡献较小，因此对权重做一定惩罚。

        ​		将文章进行单词级别的划分有时候并不是一种好的做法，比如英文中的natural language processing（自然语言处理）一词，如果将natural，language，processing这3个词拆分开来， 所表达的含义与三个词连续出现时大相径庭。通常，可以将连续出现的n个词（n≤N）组成的 词组（N-gram）也作为一个单独的特征放到向量表示中去，构成N-gram模型。另外，同一个词可能有多种词性变化，却具有相似的含义。在实际应用中，一般会对单词进行词干抽取（Word Stemming）处理，即将不同词性的单词统一成为同一词干的形式。

    * 主题模型

        主题模型用于从文本库中发现有代表性的主题（得到每个主题上面词的分布特性），并 且能够计算出每篇文章的主题分布，具体细节参见**概率图模型$\rightarrow$主题模型。**

    * 词嵌入与深度学习模型

        ​		词嵌入是一类将词向量化的模型的统称，核心思想是将每个词都映射成低维空间（通常K=50～300维）上的一个稠密向量（Dense Vector）。K维空间的每一维也可以看作一个隐含的主题，只不过不像主题模型中的主题那样直观。

        ​		由于词嵌入将每个词映射成一个K维的向量，如果一篇文档有N个词，就可以用一个N×K 维的矩阵来表示这篇文档，但是这样的表示过于底层。在实际应用中，如果仅仅把这个矩阵 作为原文本的表示特征输入到机器学习模型中，通常很难得到令人满意的结果。因此，还需要在此基础之上加工出更高层的特征。在传统的浅层机器学习模型中，一个好的特征工程往往可以带来算法效果的显著提升。而深度学习模型正好为我们提供了一种自动地进行特征工程的方式，模型中的每个隐层都可以认为对应着不同抽象层次的特征。从这个角度来讲，深度学习模型能够打败浅层模型也就顺理成章了。卷积神经网络和循环神经网络的结构在文本 表示中取得了很好的效果，主要是由于它们能够更好地对文本进行建模，抽取出一些高层的语义特征。与全连接的网络结构相比，卷积神经网络和循环神经网络一方面很好地抓住了文本的特性，另一方面又减少了网络中待学习的参数，提高了训练速度，并且降低了过拟合的风险。

## Word2Vec

WordVec是谷歌2013年提出来的，母亲是最常用的词嵌入模型之一，WordVec实际是一种浅层神经网络模型，它有两种网络结构：CBOW(Continues Bag of Words)、Skipgram

* ***知识点***

    * WordVec
    * 隐狄利克雷模型(LDA)
    * CBOW
    * Skip-gram

* ***问题***(★★★☆☆)

    * <font color=red>Word2Vec是如何工作的？它和LDA有什么区别与联系？</font>

* ***分析与解答***

    ​		CBOW的目标是根据上下文出现的词语来预测当前词的生成概率，如图a；而Skip-gram是根据当前词来预测上下文中各词的生成概率，如图b。

    ![Word2Vec的两种网络结构](.\img\Word2Vec的两种网络结构.png "Word2Vec的两种网络结构")

    ​		其中w(t)是当前所关注的词，w(t−2)、w(t−1)、w(t+1)、w(t+2)是上下文中出现的词。这里前后滑动窗口大小均设为2。

    ​		CBOW和Skip-gram都可以表示成由输入层（Input）、映射层（Projection）和输出层 （Output）组成的神经网络。

    ​		输入层中的每个词由独热编码方式表示，即所有词均表示成一个N维向量，其中N为词汇表中单词的总数。在向量中，每个词都将与之对应的维度置为1，其余维度的值均设为0。 

    ​		在映射层（又称隐含层）中，K个隐含单元（Hidden Units）的取值可以由N维输入向量以及连接输入和隐含单元之间的N×K维权重矩阵计算得到。在CBOW中，还需要将各个输入词所计算出的隐含单元求和。 

    ​		在输出层向量的值可以通过隐含层向量（K维），以及连接隐含层和输出层之间的 K×N维权重矩阵计算得到。输出层也是一个N维向量，每维与词汇表中的一个单词相对应。最后，对输出层向量应用Softmax激活函数，可以计算出每个单词的生成概率。Softmax激活函数的定义为：
    $$
    P(y=w_n|x)=\frac{e^{x_n}}{\sum\limits_{k=1}^{N}e^{x_k}}
    $$
    其中$x$代表N维的原始输出向量，$x_n$为在原始输出向量中，与单词$w_n$所对应维度的取值。

    ​		接下来的任务就是训练神经网络的权重，使得语料库中所有单词的整体生成概率最大化。从输入层到隐含层需要一个维度为N×K的权重矩阵，从隐含层到输出层又需要一个维度为K×N的权重矩阵，学习权重可以用反向传播算法实现，每次迭代时将权重沿梯度更优的方向进行一小步更新。但是由于Softmax激活函数中存在归一化项的缘故，推导出来的迭代公式需要对词汇表中的所有单词进行遍历，使得每次迭代过程非常缓慢，由此产生了Hierarchical Softmax和Negative Sampling两种改进方法。训练得到维度为N×K和K×N的两个权重矩阵之后，可以选择其中一个作为N个词的K维向量表示。

    ​		谈到Word2Vec与LDA的区别和联系，首先，LDA是利用文档中单词的共现关系来对单词按主题聚类，也可以理解为对“文档-单词”矩阵进行分解，得到“文档-主题”和“主题-单词”两个概率分布。而Word2Vec其实是对“上下文-单词”矩阵进行学习，其中上下文由周围的几个单词组成，由此得到的词向量表示更多地融入了上下文共现的特征。也就是说，如果两个单词所对应的Word2Vec向量相似度较高，那么它们很可能经常在同样的上下文中出现。需要说明的是，上述分析的是LDA与Word2Vec的不同，不应该作为主题模型和词嵌入两类方法的主要差异。主题模型通过一定的结构调整可以基于“上下文-单词”矩阵进行主题推理。同样地，词嵌入方法也可以根据“文档-单词”矩阵学习出词的隐含向量表示。主题模型和词嵌入两类方法最大的不同其实在于模型本身，主题模型是一种基于概率图模型的生成式模型，其似然函数可以写成若干条件概率连乘的形式，其中包括需要推测的隐含变量（即主题）；而 词嵌入模型一般表达为神经网络的形式，似然函数定义在网络的输出之上，需要通过学习网络的权重以得到单词的稠密向量表示。

    

## 图像数据不足时的处理办法

* ***知识点***

    * 迁移学习（Transfer Learning）
    * 生成对抗网络
    * 图像处理
    * 上采样技术
    * 数据扩充

* ***问题***(★★★☆☆)

    * <font color=red>在图像分类任务中，训练数据不足会带来什么问题？如何缓解数据量不足带来的问题？</font>

* ***分析与解答***

    ​		一个模型所能提供的信息一般来源于两个方面，一是训练数据中蕴含的信息；二是在模型的形成过程中(包括构造、学习、推理等)，人们提供的先验信息。当训练数据不足时，说明模型从原始数据中获取的信息比较少，这种情况下要想保证模型的效果，就需要更多先验信息。先验信息可以作用在模型上，例如让模型采用特定的内在结构、条件假设或添加其他一些约束条件；先验信息也可以直接施加在数据集上，即根据特定的先验假设去调整、变换或扩展训练数据，让其展现出更多的、更有用的信息，以利于后续模型的训练和学习。

    ​		具体到图像分类任务上，训练数据不足带来的问题主要表现在过拟合方面，即模型在训练样本上的效果可能不错，但在测试集上的泛化效果不佳。根据上述讨论，对应的处理方法大致也可以分两类，一是基于模型的方法，主要是采用降低过拟合风险的措施，包括简化模型（如将非线性模型简化为线性模型）、添加约束项以缩小假设空间（如$L_1/L_2$正则项）、 集成学习、Dropout超参数等；二是基于数据的方法，主要通过数据扩充（Data Augmentation），即根据一些先验知识，在保持特定信息的前提下，对原始数据进行适当变换以达到扩充数据集的效果。具体到图像分类任务中，在保持图像类别不变的前提下，可以对训练集中的每幅图像进行以下变换:

    * 一定程度内的随机旋转、平移、缩放、裁剪、填充、左右翻转等，这些变换对应着同一个目标在不同角度的观察结果。 
    * 对图像中的像素添加噪声扰动,比如椒盐噪声、高斯白噪声等。 
    * 颜色变换。例如，在图像的RGB颜色空间上进行主成分分析，得到3个主成分的特征向量$p_1,p_2,p_3$及其对应的特征值 $λ_1,λ_2,λ_3$，然后在每个像素的RGB值上添加增量 $[p_1,p_2,p_3]•[α_1λ_1,α_2λ_2,α_3λ_3]^T$，其中 $α_1,α_2,α_3$是均值为0、方差较小的高斯分布随机数。 
    * 改变图像的亮度、清晰度、对比度、锐度等。

    ​		除了直接在图像空间进行变换，还可以先对图像进行特征提取，然后在图像的特征空间内进行变换，利用一些通用的数据扩充或上采样技术，例如SMOTE(Synthetic Minority Over-sampling Technique)算法。抛开上述这些启发式的变换方法，使用生成模型也可以合成一些新样本，例如当今非常流行的生成式对抗网络模型。

    ​		此外，借助已有的其他模型或数据来进行迁移学习在深度学习中也十分常见。例如，对 于大部分图像分类任务，并不需要从头开始训练模型，而是借用一个在大规模数据集上预训 练好的通用模型，并在针对目标任务的小数据集上进行微调（fine-tune），这种微调操作就可以看成是一种简单的迁移学习。

# 模型评估

## 评价指标的局限性

在模型评估过程中，分类问题、排序问题、回归问题往往需要使用补贴的指标进行评估，在诸多评估的指标中，大部分指标只能片面地反映模型的一部分性能。

* ***知识点***

    * 准确率(Accuracy)
    * 精确率(Precision)
    * 召回率(Recall)
    * 均方根误差(Root Mean Square Error，*RMSE*)

* ***问题1***(★☆☆☆☆)

    * <font color=red>准确率的局限性</font>

* ***分析与解答***

    准确率是指分类正确的样本占总样本个数的比例：
    $$
    Accuracty = \frac{n_{correct}}{n_{total}}
    $$
    其中$n_{correct}$为被正确分类的样本个数，$n_{total}$为总样本个数。

    ​		准确率是分类问题中最简单也是最直观的评价指标，但是存在明显的缺陷，例如：当负样本站99%时，分类器把所有样本都预测为负样本也可以获得99%的准确率，所以，当不同样本比例非常不均衡时，占比大的类别往往成为影响准确率的最主要因素。

* ***问题2***(★☆☆☆☆)

    * <font color=red>精确率与召回率的权衡</font>

* ***分析与解答***

    * 精确率：分类正确的正样本个数占分类器判定为正样本的样本个数的比例

    * 召回率：分类正确的正样本个数占真正的正样本个数的比例

    通常没有一个确定的阈值把得到的结果直接判定为正样本或负样本，儿是采用Top N返回结果的Precision值和Recall值来衡量分类模型的性能，即认为模型返回的Top N 的结果就是模型判定的正样本，然后计算前N个位置上的准确率Precision N和前N个位置上的召回率Recall N
    
    ​		Precision值和Recall值是既矛盾又统一的两个指标，为了提高Precision值，分类器需要尽量在"更有把握"时才把样本预测为正样本，但是此时往往会因为过于保守而漏掉很多"没有把握"的正样本，导致Recall值降低。
    
    ​		除此之外F1-score和ROC曲线也能综合地反映一个分类模型的性能，F1-score是精确率和召回率的调和平均值：
    $$
    F1 = \frac{2\times precision\times recall}{precision + recall}
    $$
    
* ***问题3***(★☆☆☆☆)

    * <font color=red>平方根误差</font>

* ***分析与解答***

  RMSE经常被用来衡量回归模型的好坏,但是有时这个指标会失效。
  $$
  RMSE = \sqrt{\frac{\sum\limits_{i=1}^{n}(y_i -\hat{y_i})}{n}}
  $$
  其中，$y_i$是第i个样本点的真实值,$\hat{y_i}$是第$i$个样本点的预测值，n是样本点的个数
  
  ​		一般情况下，RMSE能够很好地反映回归模型预测值与真实值的偏离程度。但在实际问题中，如果存在个别偏离程度非常大的离群点（Outlier）时，即使离群点数量非常少，也会让RMSE指标变得很差。
  
  ​	**解决：**
  
  * 如果我们认定这些离群点是“噪声点”的话，就需要在数据预处理的阶段把这些噪声点过滤掉
  
  * 如果不认为这些离群点是“噪声点”的话，就需要进一步提高模型的预测能力，将离群点产生的机制建模进去
  
  * 可以找一个更合适的指标来评估该模型。关于评估指标，其实是存在比RMSE的鲁棒性更好的指标，比如平均绝对百分比误差（Mean Absolute Percent Error，MAPE），它定义为：
      $$
      MAPE = \sum\limits_{i=1}^{n}|\frac{y_i - \hat{y_i}}{y_i}|\times\frac{100}{n}
      $$
      相比RMSE，MAPE相当于把每个点的误差进行了归一化，降低了个别离群点带来的绝对误差的影响。

## ROC曲线

* ***知识点***
    
* ROC曲线
    * 曲线下的面积(Aera Under Curve，AUC)
    * P-R曲线
    
* ***问题1***(★☆☆☆☆)

    * <font color=red>什么是ROC曲线？</font>

* ***分析与解答***

    ​		ROC曲线是Receiver Operating Characteristic Curve的简称，中文名为“受试者工作特征曲 线”。ROC曲线源于军事领域，而后在医学领域应用甚广，“受试者工作特征曲线”这一名称 也正是来自于医学领域。

    ​		ROC曲线的横坐标为假阳性率（False Positive Rate，FPR）；纵坐标为真阳性率（True Positive Rate，TPR）。FPR和TPR的计算方法分别为:
    $$
    FPR=\frac{FP}{N}
    $$

    $$
    TPR = \frac{TP}{P}
    $$

    $P$是真实的正样本的数量，$N$是真实的负样本的数量，$TP$是$P$个正样本中被分类器预测为正样本的个数，$FP$是$N$个负样本中被分类器预测为正样本的个数。

* ***问题2***(★★☆☆☆)

    * <font color=red>如何计算AUC？</font>

* ***分析与解答***

    ​		AUC指的是ROC曲线下的面积大小，该值能够量化地反映基于ROC曲线衡量出的模型性能。计算AUC值只需要沿着ROC横轴做积分就可以了。由于ROC曲线一般都处于 y=x这条直线的上方*如果不是的话，只要把模型预测的概率反转成1−p就可以得到一个更好 的分类器*，所以AUC的取值一般在0.5～1之间。AUC越大，说明分类器越可能把真正的正样本排在前面，分类性能越好。

## 模型评估的方法

在机器学习中，我们通常把样本分为训练集和测试集，训练集用于训练模型，测试集用于评估模型

* ***知识点***
    
* Holdout检验
    * 交叉验证
    * 自助法(Bootstrap)
    * 微积分
    
* ***问题1***(★★☆☆☆)

    * <font color=red>在模型评估过程中，有哪些主要的验证方法，它们的优缺 点是什么?</font>

* ***分析与解答***

    * Holdout检验

        Holdout 检验是最简单也是最直接的验证方法，它将原始的样本集合随机划分成训练集 和验证集两部分.*例如:对于一个点击率预测模型，我们把样本按照 70%～30% 的比例分成两部分，70% 的样本用于模型训练；30% 的样本用于模型验证，包括绘制ROC曲线、计算精确率和召回率等指标来评估模型性能。*

        **Holdout检验的缺点：**即在验证集上计算出来的最后评估指标与原始分组有很大关系

    * 交叉检验

        ​		**k-fold交叉验证：**首先将全部样本划分成k个大小相等的样本子集；依次遍历这k个子集，每次把当前子集作为验证集，其余所有子集作为训练集，进行模型的训练和评估；最后把k次评估指标的平均值作为最终的评估指标。在实际实验中，k经常取10。

        ​		**留一验证：**每次留下1个样本作为验证集，其余所有样本作为测试集。样本总数为n，依次对n个样本进行遍历，进行n次验证，再将评估指标求平均值得到最终的评估指标。在样本总数较多的情况下，留一验证法的时间开销极大。事实上，留一验证是留p验证的特例。留p验证是每次留下p个样本作为验证集，而从n个元素中选择p个元素有$C_{n}^p$种可能，因此它的时 间开销更是远远高于留一验证，故而很少在实际工程中被应用。

    * 自助法 

        ​		不管是Holdout检验还是交叉检验，都是基于划分训练集和测试集的方法进行模型评估的。然而，当样本规模比较小时，将样本集进行划分会让训练集进一步减小，这可能会影响模型训练效果。自助法*能维持训练集样本规模的验证方法*

        ​		自助法是基于自助采样法的检验方法。对于总数为$n$的样本集合，进行$n$次有放回的随机抽样，得到大小为$n$的训练集。$n$次采样过程中，有的样本会被重复采样，有的样本没有被抽 出过，将这些没有被抽出的样本作为验证集，进行模型验证，这就是自助法的验证过程。

    * ***问题2***(★★★☆☆)

        * <font color=red>在自助法的采样过程中，对n个样本进行n次自助抽样， 当n趋于无穷大时，最终有多少数据从未被选择过？</font>

    * ***分析与解答***

        一个样本在一次抽样过程中未被抽中的概率为 $(1-\frac{1}{n})$，$n$次抽样均未抽中的$(1-\frac{1}{n})^n$概率为 。当$n$趋于无穷大时，概率为$\lim\limits_{n\rightarrow\infty}(1-\frac{1}{n})^n$。 根据重要极限$\lim \limits_{n\rightarrow\infty}(1-\frac{1}{n})^n = e$，所以有 
        $$
        \lim\limits_{n\rightarrow\infty}(1-\frac{1}{n}^n)=\lim\limits_{n\rightarrow\infty}\frac{1}{(1+\frac{1}{n-1})^n}
        $$

        $$
        ={\frac{1}{\lim\limits_{n\rightarrow\infty}(1+\frac{1}{n-1})^n }. \frac{1}{\lim\limits_{n\rightarrow\infty}(1+\frac{1}{n-1})}}
        $$

        $$
        =\frac{1}{e}\approx 0.368
        $$

        因此，当样本数很大时，大约有36.8%的样本从未被选择过，可作为验证集。

## 超参数调优

需要明确超参数搜索算法一般包括哪几个要素:**一是目标函数**,即算法需 要最大化/最小化的目标；**二是搜索范围**,一般通过上限和下限来确定;**三是算法的其他参数**,如搜索步长

* ***问题***(★☆☆☆☆)

    * <font color=red>超参数有哪些调优方法？</font>

* ***分析与解答***

    * 网格搜索

        ​		网格搜索可能是最简单、应用最广泛的超参数搜索算法，它通过查找搜索范围内的所有的点来确定最优值。如果采用较大的搜索范围以及较小的步长，网格搜索有很大概率找到全局最优值。然而，这种搜索方案十分消耗计算资源和时间，特别是需要调优的超参数比较多的时候。因此，在实际应用中，网格搜索法一般会先使用较广的搜索范围和较大的步长，来寻找全局最优值可能的位置；然后会逐渐缩小搜索范围和步长，来寻找更精确的最优值。这种操作方案可以降低所需的时间和计算量，但由于目标函数一般是非凸的，所以很可能会错过全局最优值。

    * 随机搜索

        ​		随机搜索的思想与网格搜索比较相似，只是不再测试上界和下界之间的所有值，而是在搜索范围中随机选取样本点。它的理论依据是，如果样本点集足够大，那么通过随机采样也能大概率地找到全局最优值，或其近似值。随机搜索一般会比网格搜索要快一些，但是和网格搜索的快速版一样，它的结果也是没法保证的。

    * 贝叶斯优化算法

        ​		贝叶斯优化算法在寻找最优最值参数时，采用了与网格搜索、随机搜索完全不同的方法。网格搜索和随机搜索在测试一个新点时，会忽略前一个点的信息；而贝叶斯优化算法则逸闻趣事充分利用了之前的信息。贝叶斯优化算法通过对目标函数形状进行学习，找到使目标函数向全局最优值提升的参数。具体来说，它学习目标函数形状的方法是，首先根据先验分布，假设一个搜集函数；然后，每一次使用新的采样点来测试目标函数时，利用这个信息来更新目标函数的先验分布；最后，算法测试由后验分布给出的全局最值最可能出现的位置的点。对 于贝叶斯优化算法，有一个需要注意的地方，一旦找到了一个局部最优值，它会在该区域不断采样，所以很容易陷入局部最优值。为了弥补这个缺陷，贝叶斯优化算法会在探索和利用之间找到一个平衡点，“探索”就是在还未取样的区域获取采样点；而“利用”则是根据后验分布在最可能出现全局最值的区域进行采样。

## 过拟合与欠拟合

* ***知识点***
    
* 过拟合
    * 欠拟合
    
* ***问题1***(★☆☆☆☆)

    * <font color=red>在模型评估过程中，过拟合和欠拟合具体是指什么现象？</font>

* ***分析与解答***

    **过拟合**：是指模型对于训练数据拟合呈过当的情况，反映到评估指标上，就是模型在训练 集上的表现很好，但在测试集和新数据上的表现较差。

    **欠拟合**：欠拟合指的是模型在训练和预测时表 现都不好的情况。

    ![](\img\欠拟合&欠拟合.png"欠拟合&欠拟合")

    a:是欠拟合的情况，拟合的黄线没有很好地捕捉到数据的特征，不能够很好地拟合数据。

    c:则是过拟合的情况，模型过于复杂，把噪声数据的特征也学习到模型中，导致模型泛化能力下降，在后期应用过程中很容易输出错误的预测结果。

* ***问题2***(★★☆☆☆)

    * <font color=red>能否说出几种降低过拟合和欠拟合风险的方法？</font>

* ***分析与解答***

    * 降低“过拟合”风险的方法 

        * **从数据入手，获得更多的训练数据**。使用更多的训练数据是解决过拟合问题最有效的手段，因为更多的样本能够让模型学习到更多更有效的特征，减小噪声的影响。当然，直接增加实验数据一般是很困难的，但是可以通过一定的规则来扩充训练数据。比如，在图 像分类的问题上，可以通过图像的平移、旋转、缩放等方式扩充数据；更进一步地，可以使用生成式对抗网络来合成大量的新训练数据。

        * **降低模型复杂度**。在数据较少时，模型过于复杂是产生过拟合的主要因素，适当降低模型复杂度可以避免模型拟合过多的采样噪声。例如，在神经网络模型中减少网络层数、神经元个数等；在决策树模型中降低树的深度、进行剪枝等。

        * **正则化方法**。给模型的参数加上一定的正则约束，比如将权值的大小加入到损失函数中。以$L2$正则化为例：
            $$
            C=C_0 + \frac{\lambda}{2n}.\sum\limits_{i}w_{i}^2
            $$
            这样，在优化原来的目标函数$C_0$的同时，也能避免权值过大带来的过拟合风险

        * **集成学习方法**。集成学习是把多个模型集成在一起，来降低单一模型的过拟合风险，如Bagging方法。

    * 降低“欠拟合”风险的方法

        * **添加新特征**。当特征不足或者现有特征与样本标签的相关性不强时，模型容易出现欠拟合。通过挖掘“上下文特征”“ID类特征”“组合特征”等新的特征，往往能够取得更好的效果。在深度学习潮流中，有很多模型可以帮助完成特征工程，如因子分解机、梯度提升决策树、Deep-crossing等都可以成为丰富特征的方法。
        * **增加模型复杂度**。简单模型的学习能力较差，通过增加模型的复杂度可以使模型拥有更强的拟合能力。例如，在线性模型中添加高次项，在神经网络模型中增加网络层数或神经元个数等。
        * **减小正则化系数**。正则化是用来防止过拟合的，但当模型出现欠拟合现象时，则需要有针对性地减小正则化系数。

# 经典算法

## 支持向量机

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 逻辑回归

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 决策树

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

# 降维

## PCA最大化方差理论

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## PCA最小平方误差理论

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 线性判别分析

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 线性判别分析与主成分分析

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

# 非监督学习

## K均值聚类

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 高斯混合模型

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 自组织映射神经网络

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 聚类算法的评估

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

# 概率图模型

## 概率图模型的联合概率分布

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 概率图表示

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 生成式模型与判别式模型

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 马尔可夫模型

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 主题模型

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

# 优化算法

## 有监督学习的损失函数

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 机器学习中的优化问题

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 经典优化算法

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 梯度验证

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 随机梯度下降法

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 随机梯度下降法的加速

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## L1正则化与稀疏性

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

# 采样

## 采样的作用

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 均匀分布随机数

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 常见的采样方法

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 高斯分布的采样

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 马尔可夫蒙特卡洛采样法

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 贝叶斯网络采样

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 不均衡样本的重采样

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

# 前向神经网络

## 多层感知机与布尔函数

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 深度神经网络中的激活函数

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 多层感知机的反向传播算法

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 神经网络训练技巧

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 深度卷积神经网络

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 深度残差网络

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

# 循环神经网络

## 循环升级网络和卷积神经网络

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 循环神经网络的梯度消失问题

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 循环神经网络中的激活函数

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 长短期记忆模型

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## Seq2Seq模型

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 注意力机制

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

# 强化学习

## 强化学习基础

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 视频游戏里的强化学习

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 策略梯度

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 探索与利用

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

# 集成学习

## 集成学习的种类

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 集成学习的步骤和例子

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 基分类器

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 偏差与误差

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## 梯度提升决策树的基本原理

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## XGBoost与GBDT的联系与区别

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

# 生成对抗神经网络

## 初始GANs的密码

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## WGAN：抓住低维的幽灵

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## DCGAN：当GANs遇上卷积

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## ALI：包揽推断业务

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## IRGAN：生成离散样本

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***

## SeqGAN：生成文本序列

* ***知识点***
    * 准

* ***问题1***(★☆☆☆☆)

    * <font color=red>准</font>

* ***分析与解答***



